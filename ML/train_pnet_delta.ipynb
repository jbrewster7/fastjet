{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import keras.backend as K\n",
    "import math\n",
    "import numpy as np\n",
    "import glob\n",
    "import csv\n",
    "import sys\n",
    "sys.path.append('/home/jbohm/start_tf/PointNet_Segmentation')\n",
    "from pnet_models_updated import pnet_part_seg_no_tnets, pnet_part_seg\n",
    "import awkward as ak\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"4\"\n",
    "\n",
    "# disable eager execution with tensorflow (since can't execute lambda functions with eager execution)\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA AND OUTPUT DIRS\n",
    "data_dir = '/fast_scratch_1/jbohm/cell_particle_deposit_learning/train_dirs/pnet_train_1' # parent directory that holds train, val, and test files\n",
    "train_data_dir = data_dir + '/train/'#_delta_p_pi0/'\n",
    "val_data_dir = data_dir + '/val/'#_delta_p_pi0/'\n",
    "test_data_dir = data_dir + '/test/'#_delta_p_pi0/'\n",
    "\n",
    "output_dir = \"/fast_scratch_1/jbohm/cell_particle_deposit_learning/train_dirs/pnet_train_multiclass_test/tr_25_val_5_tst_5_rho_lr_1e-2_BS_100_no_tnets\" # save model and predictions to this dir\n",
    "max_points_file = '/max_points.txt' # load the max number of points in a sample\n",
    "\n",
    "num_train_files = 25\n",
    "num_val_files = 5\n",
    "num_test_files = 5\n",
    "events_per_file = 6000 # approx number of samples in a file\n",
    "start_at_epoch = 0 # to load a pre trained model from this epoch\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 1e-2\n",
    "add_min_dist = True\n",
    "\n",
    "num_feat = 6\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA GENERATORS\n",
    "def batched_data_generator(file_names, batch_size, max_num_points, loop_infinite=True, add_min_track_dist=False):\n",
    "    while True:\n",
    "        for file in file_names:\n",
    "            point_net_data = np.load(file)\n",
    "            event_data = point_net_data['X']\n",
    "            Y = point_net_data['Y']\n",
    "            Y_int = Y.astype(np.int32)\n",
    "\n",
    "            #Y_int[Y_int == 2] = 1 # for delta+ -> p + pi0 (to run with 2 classes - 0 as p and 1 as pi0)\n",
    "\n",
    "            identity = np.eye(num_classes)\n",
    "            one_hot = np.concatenate([identity, np.negative(np.ones((1, num_classes)))])\n",
    "\n",
    "            # pad X and Y data to have y dimension of max_num_points\n",
    "            X_padded = np.zeros((event_data.shape[0], max_num_points, num_feat)) # pad with zeros\n",
    "            Y_padded = np.negative(np.ones(((event_data.shape[0], max_num_points, num_classes)))) # pad with -1s\n",
    "            \n",
    "            for i, event in enumerate(event_data): \n",
    "                X_padded[i, :len(event), :5] = event\n",
    "                Y_padded[i, :len(event), :] = one_hot[np.squeeze(Y_int[i])]\n",
    "\n",
    "                # add a feature of min distance from cell point to track point (for track points this is 0)\n",
    "                if add_min_track_dist:\n",
    "                    track_points_idx = np.arange(len(event))[event[:, 4] == 1]\n",
    "                    dists = np.zeros((len(event), len(track_points_idx)))\n",
    "\n",
    "                    for j, track_point_idx in enumerate(track_points_idx):\n",
    "                        dists[:, j] = np.sqrt((event[:, 1] - event[track_point_idx, 1])**2 + (event[:, 2] - event[track_point_idx, 2])**2 + (event[:, 3] - event[track_point_idx, 3])**2)\n",
    "\n",
    "                    dist_feat_idx = 5\n",
    "                    if np.any(track_points_idx):\n",
    "                        min_dists = np.min(dists, axis=1)\n",
    "                        # recast padding to 0 (padding is where the point is not a track and the label is -1)\n",
    "                        min_dists[(event[:, 4] == 0) & (Y[i, :, 0] == -1)] = 0#13500\n",
    "                        X_padded[i, :len(event), dist_feat_idx] = min_dists\n",
    "    \n",
    "            # split into batch_size groups of events\n",
    "            for i in range(1, math.ceil(event_data.shape[0]/batch_size)):\n",
    "                yield X_padded[(i-1)*batch_size:i*batch_size], Y_padded[(i-1)*batch_size:i*batch_size]\n",
    "\n",
    "        if not loop_infinite:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS\n",
    "def masked_ce_pointwise_loss(y_true, y_pred):\n",
    "    mask = tf.reduce_all(tf.equal(y_true, -1), axis=2)\n",
    "    cell_ce = K.categorical_crossentropy(y_pred, y_true, axis=2)\n",
    "    return K.sum(tf.where(mask, tf.zeros_like(cell_ce), cell_ce)) / K.sum(tf.cast(tf.logical_not(mask), tf.float32), axis=None)\n",
    "\n",
    "def masked_bce_pointwise_loss(y_true, y_pred):\n",
    "    y_true = tf.expand_dims(y_true[:,:,0], -1)\n",
    "    mask = tf.cast(tf.not_equal(y_true, -1), tf.float32)\n",
    "    return K.sum(K.binary_crossentropy(tf.multiply(y_pred, mask), tf.multiply(y_true, mask)), axis=None) / K.sum(mask, axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP TRAIN, VAL, AND TEST GENERATORS\n",
    "train_files = np.sort(glob.glob(train_data_dir+'*.npz'))[:num_train_files]\n",
    "val_files = np.sort(glob.glob(val_data_dir+'*.npz'))[:num_val_files]\n",
    "test_files = np.sort(glob.glob(test_data_dir+'*.npz'))[:num_test_files]\n",
    "\n",
    "num_batches_train = (len(train_files) * events_per_file) / BATCH_SIZE \n",
    "num_batches_val = (len(val_files) * events_per_file) / BATCH_SIZE\n",
    "num_batches_test = (len(test_files) * events_per_file) / BATCH_SIZE\n",
    "\n",
    "# load the max number of points (N) - saved to data dir\n",
    "with open(data_dir + max_points_file) as f:\n",
    "    N = int(f.readline())\n",
    "\n",
    "train_generator = batched_data_generator(train_files, BATCH_SIZE, N, add_min_track_dist=add_min_dist)\n",
    "val_generator = batched_data_generator(val_files, BATCH_SIZE, N, add_min_track_dist=add_min_dist)\n",
    "test_generator = batched_data_generator(test_files, BATCH_SIZE, N, loop_infinite=False, add_min_track_dist=add_min_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, None, 6)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "features_64_tdist (TimeDistribu (None, None, 64)     448         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchNorm_features_64 (MaskedBa (None, None, 64)     128         features_64_tdist[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "features_64_relu (Activation)   (None, None, 64)     0           batchNorm_features_64[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "features_128_1_tdist (TimeDistr (None, None, 128)    8320        features_64_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batchNorm_features_128_1 (Maske (None, None, 128)    256         features_128_1_tdist[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "features_128_1_relu (Activation (None, None, 128)    0           batchNorm_features_128_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "features_128_2_tdist (TimeDistr (None, None, 128)    16512       features_128_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batchNorm_features_128_2 (Maske (None, None, 128)    256         features_128_2_tdist[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "features_128_2_relu (Activation (None, None, 128)    0           batchNorm_features_128_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "features_512_tdist (TimeDistrib (None, None, 512)    66048       features_128_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batchNorm_features_512 (MaskedB (None, None, 512)    1024        features_512_tdist[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "features_512_relu (Activation)  (None, None, 512)    0           batchNorm_features_512[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pre_maxpool_block_tdist (TimeDi (None, None, 2048)   1050624     features_512_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batchNorm_pre_maxpool_block (Ma (None, None, 2048)   4096        pre_maxpool_block_tdist[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "pre_maxpool_block_relu (Activat (None, None, 2048)   0           batchNorm_pre_maxpool_block[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "pre_maxpool_block_masked (Lambd (None, None, 2048)   0           pre_maxpool_block_relu[0][0]     \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_features (MaxPooling1D)  (None, None, 2048)   0           pre_maxpool_block_masked[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_3 (TensorFlowO [(None, None, 2048)] 0           global_features[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "segmentation_input (Concatenate (None, None, 2880)   0           features_64_relu[0][0]           \n",
      "                                                                 features_128_1_relu[0][0]        \n",
      "                                                                 features_128_2_relu[0][0]        \n",
      "                                                                 features_512_relu[0][0]          \n",
      "                                                                 tf_op_layer_Tile_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "segmentation_features_tdist (Ti (None, None, 128)    368768      segmentation_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batchNorm_segmentation_features (None, None, 128)    256         segmentation_features_tdist[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "segmentation_features_relu (Act (None, None, 128)    0           batchNorm_segmentation_features[0\n",
      "__________________________________________________________________________________________________\n",
      "last_tdist (TimeDistributed)    (None, None, 2)      258         segmentation_features_relu[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "last_act (Activation)           (None, None, 2)      0           last_tdist[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,516,994\n",
      "Trainable params: 1,516,994\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# COMPILE MODEL\n",
    "model = pnet_part_seg_no_tnets(N, num_feat, num_classes)\n",
    "\n",
    "# if resuming training load saved weights\n",
    "if start_at_epoch:\n",
    "    model.load_weights(output_dir + \"/weights/weights_\" + str(start_at_epoch - 1) + \".h5\")\n",
    "\n",
    "model.compile(loss=masked_ce_pointwise_loss, optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALLBACKS\n",
    "# make directories if not present\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "if not os.path.exists(output_dir + \"/weights\"):\n",
    "    os.makedirs(output_dir + \"/weights\")\n",
    "if not os.path.exists(output_dir + \"/tests\"):\n",
    "    os.makedirs(output_dir + \"/tests\")\n",
    "\n",
    "# save preds, model weights, and train/val loss after each epoch\n",
    "class SaveEpoch(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # save preds on new test file\n",
    "        per_epoch_test_generator = batched_data_generator(test_files, BATCH_SIZE, N, loop_infinite=False, add_min_track_dist=add_min_dist)\n",
    "\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        for X_test, Y_test in per_epoch_test_generator:\n",
    "            predictions.extend(model.predict(X_test))\n",
    "            labels.extend(Y_test)\n",
    "        np.save(output_dir + \"/tests/preds_\" + str(start_at_epoch + epoch) + \".npy\", predictions)\n",
    "        if epoch == 0:\n",
    "            # save the labels for up to 5 test files\n",
    "            np.save(output_dir + \"/tests/labels.npy\", labels)\n",
    "\n",
    "        # save model weights\n",
    "        model.save_weights(output_dir + \"/weights/weights_\" + str(start_at_epoch + epoch) + \".h5\")\n",
    "        # save loss\n",
    "        with open(output_dir + \"/log_loss.csv\" ,'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([start_at_epoch + epoch , logs[\"loss\"], logs[\"val_loss\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please specify the `validation_steps` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet_delta.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet_delta.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_generator,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet_delta.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mEPOCHS,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet_delta.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_generator,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet_delta.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet_delta.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49mnum_batches_train,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet_delta.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mnum_batches_val,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet_delta.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[SaveEpoch()])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:777\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_call_args(\u001b[39m'\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    776\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_training_loop(x)\n\u001b[0;32m--> 777\u001b[0m \u001b[39mreturn\u001b[39;00m func\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    778\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    779\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    780\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    781\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    782\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    783\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    784\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    785\u001b[0m     validation_split\u001b[39m=\u001b[39;49mvalidation_split,\n\u001b[1;32m    786\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[1;32m    787\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    788\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[1;32m    789\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    790\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[1;32m    791\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m    792\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m    793\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[1;32m    794\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m    795\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m    796\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training_generator_v1.py:570\u001b[0m, in \u001b[0;36mGeneratorOrSequenceTrainingLoop.fit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    567\u001b[0m model\u001b[39m.\u001b[39m_validate_or_infer_batch_size(batch_size, steps_per_epoch, x)\n\u001b[1;32m    568\u001b[0m training_utils_v1\u001b[39m.\u001b[39mcheck_generator_arguments(\n\u001b[1;32m    569\u001b[0m     y, sample_weight, validation_split\u001b[39m=\u001b[39mvalidation_split)\n\u001b[0;32m--> 570\u001b[0m \u001b[39mreturn\u001b[39;00m fit_generator(\n\u001b[1;32m    571\u001b[0m     model,\n\u001b[1;32m    572\u001b[0m     x,\n\u001b[1;32m    573\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m    574\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    575\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    576\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    577\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[1;32m    578\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m    579\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[1;32m    580\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[1;32m    581\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m    582\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m    583\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m    584\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    585\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[1;32m    586\u001b[0m     steps_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msteps_per_epoch\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training_generator_v1.py:142\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m do_validation \u001b[39m=\u001b[39m validation_data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    141\u001b[0m is_sequence \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(generator, data_utils\u001b[39m.\u001b[39mSequence)\n\u001b[0;32m--> 142\u001b[0m _validate_arguments(is_sequence, is_dataset, use_multiprocessing, workers,\n\u001b[1;32m    143\u001b[0m                     steps_per_epoch, validation_data, validation_steps, mode,\n\u001b[1;32m    144\u001b[0m                     kwargs)\n\u001b[1;32m    146\u001b[0m batch_function \u001b[39m=\u001b[39m _make_execution_function(\n\u001b[1;32m    147\u001b[0m     model, mode, class_weight\u001b[39m=\u001b[39mclass_weight)\n\u001b[1;32m    149\u001b[0m \u001b[39m# Create the queue for the generator.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training_generator_v1.py:411\u001b[0m, in \u001b[0;36m_validate_arguments\u001b[0;34m(is_sequence, is_dataset, use_multiprocessing, workers, steps_per_epoch, validation_data, validation_steps, mode, kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m val_gen \u001b[39m=\u001b[39m (\n\u001b[1;32m    407\u001b[0m     data_utils\u001b[39m.\u001b[39mis_generator_or_sequence(validation_data) \u001b[39mor\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     \u001b[39misinstance\u001b[39m(validation_data, tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mIterator))\n\u001b[1;32m    409\u001b[0m \u001b[39mif\u001b[39;00m (val_gen \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(validation_data, data_utils\u001b[39m.\u001b[39mSequence) \u001b[39mand\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[39mnot\u001b[39;00m validation_steps):\n\u001b[0;32m--> 411\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mPlease specify the `validation_steps` argument.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    413\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(k \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msteps\u001b[39m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m kwargs):\n\u001b[1;32m    414\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInvalid arguments passed: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    415\u001b[0m       [k \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m kwargs \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msteps\u001b[39m\u001b[39m'\u001b[39m]))\n",
      "\u001b[0;31mValueError\u001b[0m: Please specify the `validation_steps` argument."
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    verbose=1,\n",
    "    steps_per_epoch=num_batches_train,\n",
    "    validation_steps=num_batches_val,\n",
    "    callbacks=[SaveEpoch()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "/fast_scratch_1/jbohm/cell_particle_deposit_learning/train_dirs/pnet_train_delta_fixed/tr_25_val_5_tst_5_rho_lr_1e-2_BS_100_no_tnets/log_loss.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet_delta.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet_delta.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m model_file_path \u001b[39m=\u001b[39m output_dir \u001b[39m# \"/fast_scratch_1/jbohm/cell_particle_deposit_learning/train_dirs/pnet_train_1/tr_50_val_5_tst_5_lr_1e-2_BS_100_no_tnets_1_track_add_min_dist/\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet_delta.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# plot train and val loss\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet_delta.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m loss_data \u001b[39m=\u001b[39m genfromtxt(model_file_path \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m/log_loss.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, delimiter\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mtranspose()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet_delta.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_data[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet_delta.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m val_loss \u001b[39m=\u001b[39m loss_data[\u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py:1939\u001b[0m, in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[1;32m   1937\u001b[0m     fname \u001b[39m=\u001b[39m os_fspath(fname)\n\u001b[1;32m   1938\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fname, \u001b[39mstr\u001b[39m):\n\u001b[0;32m-> 1939\u001b[0m     fid \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlib\u001b[39m.\u001b[39;49m_datasource\u001b[39m.\u001b[39;49mopen(fname, \u001b[39m'\u001b[39;49m\u001b[39mrt\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49mencoding)\n\u001b[1;32m   1940\u001b[0m     fid_ctx \u001b[39m=\u001b[39m contextlib\u001b[39m.\u001b[39mclosing(fid)\n\u001b[1;32m   1941\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m ds \u001b[39m=\u001b[39m DataSource(destpath)\n\u001b[0;32m--> 193\u001b[0m \u001b[39mreturn\u001b[39;00m ds\u001b[39m.\u001b[39;49mopen(path, mode, encoding\u001b[39m=\u001b[39;49mencoding, newline\u001b[39m=\u001b[39;49mnewline)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[39m=\u001b[39mmode,\n\u001b[1;32m    531\u001b[0m                               encoding\u001b[39m=\u001b[39mencoding, newline\u001b[39m=\u001b[39mnewline)\n\u001b[1;32m    532\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 533\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m not found.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: /fast_scratch_1/jbohm/cell_particle_deposit_learning/train_dirs/pnet_train_delta_fixed/tr_25_val_5_tst_5_rho_lr_1e-2_BS_100_no_tnets/log_loss.csv not found."
     ]
    }
   ],
   "source": [
    "def get_accuracy(preds, labels):\n",
    "    correct = (np.array(preds) > 0.5) == np.array(labels)\n",
    "    return np.count_nonzero(correct)/len(correct)\n",
    "\n",
    "def get_accuracy_multi_class(preds, labels):\n",
    "    correct = np.argmax(labels, axis=1) == np.argmax(preds, axis=1)\n",
    "    return np.count_nonzero(correct)/len(correct)\n",
    "\n",
    "# plot train and val loss over epoch\n",
    "def plot_train_and_val_loss(train_loss, val_loss):\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(train_loss, label=\"train\", color=\"#009fdf\")\n",
    "    plt.plot(val_loss, label=\"tal\", color=\"#F2705D\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy(preds, labels):\n",
    "    accuracy = []\n",
    "    for epoch_preds in preds:\n",
    "        accuracy.append(get_accuracy_multi_class(epoch_preds, labels))\n",
    "    print(accuracy)\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(accuracy, color=\"#009fdf\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Classification accuracy\")\n",
    "    plt.show()\n",
    "\n",
    "num_epoch = 3\n",
    "model_file_path = output_dir # \"/fast_scratch_1/jbohm/cell_particle_deposit_learning/train_dirs/pnet_train_1/tr_50_val_5_tst_5_lr_1e-2_BS_100_no_tnets_1_track_add_min_dist/\"\n",
    "\n",
    "# plot train and val loss\n",
    "loss_data = genfromtxt(model_file_path + \"/log_loss.csv\", delimiter=',').transpose()\n",
    "loss = loss_data[1]\n",
    "val_loss = loss_data[2]\n",
    "plot_train_and_val_loss(loss, val_loss)\n",
    "\n",
    "# plot test accuracy\n",
    "labels_unmasked = ak.Array(np.load(model_file_path + \"/tests/labels.npy\"))\n",
    "labels = labels_unmasked[labels_unmasked[:,:,0] != -1]\n",
    "\n",
    "print(\"num points unmasked:\", np.count_nonzero([labels_unmasked[:,:,0] != -1]))\n",
    "print(\"num points masked:\", np.count_nonzero([labels_unmasked[:,:,0] == -1]))\n",
    "preds = []\n",
    "for epoch in range(num_epoch):\n",
    "    preds_unmasked = ak.Array(np.load(model_file_path + \"/tests/preds_\" + str(epoch) + \".npy\"))\n",
    "    #preds_unmasked = preds_unmasked[:, :, 0]\n",
    "    epoch_preds = preds_unmasked[labels_unmasked[:,:,0] != -1]\n",
    "    preds.append(epoch_preds)\n",
    "\n",
    "plot_accuracy(preds, labels)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.6046729871340655\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(np.argmax(epoch_preds, axis=1) == 0)/len(epoch_preds))\n",
    "print(np.count_nonzero(np.argmax(labels, axis=1) == 0)/len(labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
